{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8sMfeEiD9Kx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "from typing import List, Tuple, Dict, Any, Optional\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "from sqlalchemy import create_engine, text, inspect\n",
        "\n",
        "from pymilvus import connections, utility, FieldSchema, CollectionSchema, DataType, Collection\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "import google.generativeai as genai\n",
        "from google.api_core.exceptions import TooManyRequests, ServiceUnavailable\n",
        "\n",
        "try:\n",
        "    from zoneinfo import ZoneInfo\n",
        "    _DHAKA_TZ = ZoneInfo(\"Asia/Dhaka\")\n",
        "except Exception:\n",
        "    _DHAKA_TZ = None\n",
        "\n",
        "\n",
        "def build_engine() -> Any:\n",
        "    real_host = os.getenv(\"REAL_HOST\", \"\")\n",
        "    real_port = os.getenv(\"REAL_PORT\", \"\")\n",
        "    password = os.getenv(\"PG_PASSWORD\", \"\")\n",
        "    db_name = os.getenv(\"PG_DB\", \"hospital\")\n",
        "    user = os.getenv(\"PG_USER\", \"postgres\")\n",
        "\n",
        "    if not real_host or not real_port or not password:\n",
        "        raise ValueError(\"Missing Postgres env vars REAL_HOST, REAL_PORT, PG_PASSWORD\")\n",
        "\n",
        "    database_url = f\"postgresql://{user}:{password}@{real_host}:{real_port}/{db_name}\"\n",
        "    return create_engine(\n",
        "        database_url,\n",
        "        pool_pre_ping=True,\n",
        "        connect_args={\"connect_timeout\": 10},\n",
        "    )\n",
        "\n",
        "\n",
        "def build_milvus_collection(collection_name: str, embed_dim: int) -> Collection:\n",
        "    uri = os.getenv(\"MILVUS_URI\", \"\")\n",
        "    token = os.getenv(\"MILVUS_TOKEN\", \"\")\n",
        "\n",
        "    if not uri or not token:\n",
        "        raise ValueError(\"Missing Milvus env vars MILVUS_URI, MILVUS_TOKEN\")\n",
        "\n",
        "    connections.connect(alias=\"default\", uri=uri, token=token)\n",
        "\n",
        "    expected_fields = [\n",
        "        \"id\", \"embedding\", \"doc_type\", \"table_name\",\n",
        "        \"contains_phi\", \"hospital_scoped\", \"schema_version\", \"text\"\n",
        "    ]\n",
        "\n",
        "    if utility.has_collection(collection_name):\n",
        "        col = Collection(collection_name)\n",
        "        existing_fields = [f.name for f in col.schema.fields]\n",
        "        if existing_fields != expected_fields:\n",
        "            utility.drop_collection(collection_name)\n",
        "        else:\n",
        "            col.load()\n",
        "            return col\n",
        "\n",
        "    fields = [\n",
        "        FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
        "        FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=embed_dim),\n",
        "        FieldSchema(name=\"doc_type\", dtype=DataType.VARCHAR, max_length=32),\n",
        "        FieldSchema(name=\"table_name\", dtype=DataType.VARCHAR, max_length=128),\n",
        "        FieldSchema(name=\"contains_phi\", dtype=DataType.BOOL),\n",
        "        FieldSchema(name=\"hospital_scoped\", dtype=DataType.BOOL),\n",
        "        FieldSchema(name=\"schema_version\", dtype=DataType.VARCHAR, max_length=64),\n",
        "        FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=8192),\n",
        "    ]\n",
        "    schema = CollectionSchema(fields, description=\"Schema docs for SQL generation\")\n",
        "    col = Collection(collection_name, schema=schema)\n",
        "\n",
        "    index_params = {\n",
        "        \"index_type\": \"HNSW\",\n",
        "        \"metric_type\": \"COSINE\",\n",
        "        \"params\": {\"M\": 16, \"efConstruction\": 200},\n",
        "    }\n",
        "    col.create_index(field_name=\"embedding\", index_params=index_params)\n",
        "    col.load()\n",
        "    return col\n",
        "\n",
        "\n",
        "def build_embedder() -> Any:\n",
        "    return SentenceTransformer(\"intfloat/e5-base-v2\")\n",
        "\n",
        "\n",
        "def embed_text(embedder: Any, s: str) -> List[float]:\n",
        "    vec = embedder.encode([s], normalize_embeddings=True)[0]\n",
        "    return vec.tolist()\n",
        "\n",
        "\n",
        "def _common_joins_for_table(table_name: str) -> List[str]:\n",
        "    join_map = {\n",
        "        \"admissions\": [\n",
        "            \"admissions.patient_id -> patients.patient_id\",\n",
        "            \"admissions.department_id -> departments.department_id\",\n",
        "            \"admissions.ward_id -> wards.ward_id\",\n",
        "            \"admissions.room_id -> rooms.room_id\",\n",
        "            \"admissions.bed_id -> beds.bed_id\",\n",
        "            \"admissions.attending_doctor_id -> staff.staff_id\",\n",
        "        ],\n",
        "        \"discharges\": [\"discharges.admission_id -> admissions.admission_id\"],\n",
        "        \"diagnoses\": [\"diagnoses.admission_id -> admissions.admission_id\"],\n",
        "        \"patient_conditions\": [\"patient_conditions.admission_id -> admissions.admission_id\"],\n",
        "        \"patient_vitals\": [\"patient_vitals.admission_id -> admissions.admission_id\"],\n",
        "        \"prescriptions\": [\n",
        "            \"prescriptions.admission_id -> admissions.admission_id\",\n",
        "            \"prescriptions.prescribed_by_staff_id -> staff.staff_id\",\n",
        "        ],\n",
        "        \"prescription_items\": [\n",
        "            \"prescription_items.prescription_id -> prescriptions.prescription_id\",\n",
        "            \"prescription_items.medication_id -> medications.medication_id\",\n",
        "        ],\n",
        "        \"nurse_assignments\": [\n",
        "            \"nurse_assignments.admission_id -> admissions.admission_id\",\n",
        "            \"nurse_assignments.nurse_staff_id -> staff.staff_id\",\n",
        "        ],\n",
        "        \"births\": [\n",
        "            \"births.mother_patient_id -> patients.patient_id\",\n",
        "            \"births.admission_id -> admissions.admission_id\",\n",
        "            \"births.doctor_staff_id -> staff.staff_id\",\n",
        "        ],\n",
        "        \"newborns\": [\"newborns.birth_id -> births.birth_id\"],\n",
        "        \"abortion_cases\": [\n",
        "            \"abortion_cases.patient_id -> patients.patient_id\",\n",
        "            \"abortion_cases.admission_id -> admissions.admission_id\",\n",
        "            \"abortion_cases.performed_by_staff_id -> staff.staff_id\",\n",
        "        ],\n",
        "        \"donations\": [\n",
        "            \"donations.donor_id -> donors.donor_id\",\n",
        "            \"donations.recipient_patient_id -> patients.patient_id\",\n",
        "        ],\n",
        "        \"organ_donation_items\": [\n",
        "            \"organ_donation_items.donation_id -> donations.donation_id\",\n",
        "            \"organ_donation_items.organ_id -> organs.organ_id\",\n",
        "        ],\n",
        "        \"surgeries\": [\"surgeries.admission_id -> admissions.admission_id\"],\n",
        "        \"surgery_team\": [\n",
        "            \"surgery_team.surgery_id -> surgeries.surgery_id\",\n",
        "            \"surgery_team.staff_id -> staff.staff_id\",\n",
        "        ],\n",
        "        \"blood_bank_inventory\": [\n",
        "            \"blood_bank_inventory.blood_group_id -> blood_groups.blood_group_id\"\n",
        "        ],\n",
        "        \"staff_department\": [\n",
        "            \"staff_department.staff_id -> staff.staff_id\",\n",
        "            \"staff_department.department_id -> departments.department_id\",\n",
        "        ],\n",
        "        \"staff_shifts\": [\n",
        "            \"staff_shifts.staff_id -> staff.staff_id\",\n",
        "            \"staff_shifts.department_id -> departments.department_id\",\n",
        "        ],\n",
        "    }\n",
        "    return join_map.get(table_name, [])\n",
        "\n",
        "\n",
        "def build_table_doc(inspector, table_name: str) -> str:\n",
        "    cols = inspector.get_columns(table_name)\n",
        "    pk = inspector.get_pk_constraint(table_name).get(\"constrained_columns\", [])\n",
        "    fks = inspector.get_foreign_keys(table_name)\n",
        "\n",
        "    lines = []\n",
        "    lines.append(\"doc_type: table\")\n",
        "    lines.append(f\"table: {table_name}\")\n",
        "    lines.append(f\"primary_key: {', '.join(pk) if pk else 'none'}\")\n",
        "    lines.append(\"columns:\")\n",
        "    for c in cols:\n",
        "        nn = \"not null\" if not c.get(\"nullable\", True) else \"nullable\"\n",
        "        lines.append(f\"  {c['name']} {str(c['type'])} {nn}\")\n",
        "    lines.append(\"foreign_keys:\")\n",
        "    if not fks:\n",
        "        lines.append(\"  none\")\n",
        "    else:\n",
        "        for fk in fks:\n",
        "            src_cols = \", \".join(fk.get(\"constrained_columns\", []))\n",
        "            ref_table = fk.get(\"referred_table\")\n",
        "            ref_cols = \", \".join(fk.get(\"referred_columns\", []))\n",
        "            lines.append(f\"  {table_name}.{src_cols} -> {ref_table}.{ref_cols}\")\n",
        "\n",
        "    joins = _common_joins_for_table(table_name)\n",
        "    if joins:\n",
        "        lines.append(\"common_joins:\")\n",
        "        for j in joins:\n",
        "            lines.append(f\"  {j}\")\n",
        "\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "def table_contains_phi(table_name: str) -> bool:\n",
        "    return table_name in {\n",
        "        \"patients\",\n",
        "        \"admissions\",\n",
        "        \"discharges\",\n",
        "        \"diagnoses\",\n",
        "        \"patient_conditions\",\n",
        "        \"patient_vitals\",\n",
        "        \"prescriptions\",\n",
        "        \"prescription_items\",\n",
        "        \"nurse_assignments\",\n",
        "        \"births\",\n",
        "        \"newborns\",\n",
        "        \"abortion_cases\",\n",
        "        \"donations\",\n",
        "        \"donors\",\n",
        "        \"surgeries\",\n",
        "        \"surgery_team\",\n",
        "        \"organ_donation_items\",\n",
        "    }\n",
        "\n",
        "\n",
        "def index_schema(engine, schema_col: Collection, embedder, schema_version: str = \"v1\") -> int:\n",
        "    inspector = inspect(engine)\n",
        "    tables = inspector.get_table_names()\n",
        "\n",
        "    schema_col.delete(expr=\"id >= 0\")\n",
        "    schema_col.flush()\n",
        "\n",
        "    embeddings = []\n",
        "    doc_type = []\n",
        "    table_name = []\n",
        "    contains_phi = []\n",
        "    hospital_scoped = []\n",
        "    schema_versions = []\n",
        "    text_docs = []\n",
        "\n",
        "    for t in tables:\n",
        "        doc = build_table_doc(inspector, t)\n",
        "        emb = embed_text(embedder, doc)\n",
        "\n",
        "        embeddings.append(emb)\n",
        "        doc_type.append(\"table\")\n",
        "        table_name.append(t)\n",
        "        contains_phi.append(bool(table_contains_phi(t)))\n",
        "        hospital_scoped.append(\"hospital_id\" in [c[\"name\"] for c in inspector.get_columns(t)])\n",
        "        schema_versions.append(schema_version)\n",
        "        text_docs.append(doc)\n",
        "\n",
        "    schema_col.insert([embeddings, doc_type, table_name, contains_phi, hospital_scoped, schema_versions, text_docs])\n",
        "    schema_col.flush()\n",
        "    return len(tables)\n",
        "\n",
        "\n",
        "def retrieve_schema(schema_col: Collection, embedder, question: str, top_k: int = 12) -> List[Tuple[str, str]]:\n",
        "    q_emb = embed_text(embedder, question)\n",
        "    res = schema_col.search(\n",
        "        data=[q_emb],\n",
        "        anns_field=\"embedding\",\n",
        "        param={\"params\": {\"ef\": 128}},\n",
        "        limit=top_k,\n",
        "        output_fields=[\"table_name\", \"text\"],\n",
        "    )\n",
        "    return [(h.entity.get(\"table_name\"), h.entity.get(\"text\")) for h in res[0]]\n",
        "\n",
        "\n",
        "def _ensure_tables(schema_col: Collection, embedder, schema_hits: List[Tuple[str, str]], must_have: List[str]) -> List[Tuple[str, str]]:\n",
        "    have = {t for t, _ in schema_hits}\n",
        "    out = list(schema_hits)\n",
        "    for t in must_have:\n",
        "        if t in have:\n",
        "            continue\n",
        "        extra = retrieve_schema(schema_col, embedder, f\"table: {t}\", top_k=3)\n",
        "        for et, edoc in extra:\n",
        "            if et == t and et not in have and edoc:\n",
        "                out.append((et, edoc))\n",
        "                have.add(et)\n",
        "                break\n",
        "    return out\n",
        "\n",
        "\n",
        "def build_schema_context(schema_hits: List[Tuple[str, str]]) -> str:\n",
        "    return \"\\n\\n\".join([doc for _, doc in schema_hits if doc])\n",
        "\n",
        "\n",
        "def safe_generate(model, prompt: str, max_retries: int = 8) -> str:\n",
        "    last_err = None\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            resp = model.generate_content(prompt, request_options={\"timeout\": 120})\n",
        "            return (resp.text or \"\").strip()\n",
        "        except (TooManyRequests, ServiceUnavailable) as e:\n",
        "            last_err = e\n",
        "            time.sleep(min(60, (2 ** attempt) + 0.25))\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "            msg = str(e)\n",
        "            if (\"RemoteDisconnected\" in msg) or (\"Connection aborted\" in msg) or (\"EOF\" in msg):\n",
        "                time.sleep(min(60, (2 ** attempt) + 0.5))\n",
        "                continue\n",
        "            raise\n",
        "    raise RuntimeError(f\"LLM failed after retries. Last error was: {last_err}\")\n",
        "\n",
        "\n",
        "SQL_CACHE: Dict[str, str] = {}\n",
        "_LAST_LLM_CALL_TS = 0.0\n",
        "\n",
        "FORBIDDEN = {\n",
        "    \"insert\", \"update\", \"delete\", \"drop\", \"alter\", \"truncate\", \"create\",\n",
        "    \"grant\", \"revoke\", \"vacuum\", \"copy\"\n",
        "}\n",
        "\n",
        "\n",
        "def sanitize_sql(sql: str) -> str:\n",
        "    s = (sql or \"\").strip()\n",
        "    s = re.sub(r\"^```[a-zA-Z]*\\s*\", \"\", s).strip()\n",
        "    s = re.sub(r\"\\s*```$\", \"\", s).strip()\n",
        "    s = re.sub(r\";+\\s*$\", \"\", s).strip()\n",
        "    return s\n",
        "\n",
        "\n",
        "def extract_cte_names(sql: str) -> set:\n",
        "    s = re.sub(r\"\\s+\", \" \", (sql or \"\").strip()).strip().lower()\n",
        "    if not s.startswith(\"with \"):\n",
        "        return set()\n",
        "\n",
        "    ctes = set()\n",
        "    for m in re.finditer(r\"\\bwith\\s+([a-zA-Z_]\\w*)\\s+as\\s*\\(|\\)\\s*,\\s*([a-zA-Z_]\\w*)\\s+as\\s*\\(\", s):\n",
        "        if m.group(1):\n",
        "            ctes.add(m.group(1))\n",
        "        if m.group(2):\n",
        "            ctes.add(m.group(2))\n",
        "    return ctes\n",
        "\n",
        "\n",
        "def extract_table_names(sql: str) -> set:\n",
        "    s = re.sub(r\"\\s+\", \" \", (sql or \"\").strip().lower())\n",
        "    ctes = extract_cte_names(sql)\n",
        "\n",
        "    tables = set()\n",
        "    for m in re.finditer(r\"\\b(from|join)\\s+([a-zA-Z_][\\w]*)\", s):\n",
        "        t = m.group(2)\n",
        "        t = t.split(\".\")[-1].strip('\"')\n",
        "        if t and (t not in ctes):\n",
        "            tables.add(t)\n",
        "    return tables\n",
        "\n",
        "\n",
        "def validate_sql(sql: str, allowed_tables: List[str]):\n",
        "    s = (sql or \"\").strip()\n",
        "    if not s:\n",
        "        raise ValueError(\"Empty SQL\")\n",
        "    if \";\" in s:\n",
        "        raise ValueError(\"Semicolons not allowed\")\n",
        "    low = s.lower()\n",
        "    if not (low.startswith(\"select\") or low.startswith(\"with\")):\n",
        "        raise ValueError(\"Only SELECT or WITH allowed\")\n",
        "    for kw in FORBIDDEN:\n",
        "        if re.search(rf\"\\b{kw}\\b\", low):\n",
        "            raise ValueError(\"Forbidden SQL keyword found\")\n",
        "    if \":hospital_id\" not in s:\n",
        "        raise ValueError(\"Missing required :hospital_id parameter\")\n",
        "\n",
        "    used = extract_table_names(s)\n",
        "    unknown = used.difference(set(allowed_tables))\n",
        "    if unknown:\n",
        "        raise ValueError(f\"SQL references tables not in retrieved schema: {sorted(list(unknown))}\")\n",
        "\n",
        "\n",
        "def extract_alias_map(sql: str) -> Dict[str, str]:\n",
        "    s = re.sub(r\"\\s+\", \" \", sql.strip())\n",
        "    alias_map = {}\n",
        "    pattern = re.compile(r\"\\b(from|join)\\s+([a-zA-Z_][\\w]*)\\s*(?:as\\s+)?([a-zA-Z_][\\w]*)?\\b\", re.IGNORECASE)\n",
        "    for m in pattern.finditer(s):\n",
        "        table = m.group(2)\n",
        "        alias = m.group(3) or table\n",
        "        alias_map[alias] = table\n",
        "    return alias_map\n",
        "\n",
        "\n",
        "def get_table_columns(engine, table_name: str) -> set:\n",
        "    inspector = inspect(engine)\n",
        "    return {c[\"name\"] for c in inspector.get_columns(table_name)}\n",
        "\n",
        "\n",
        "def _parse_qualified_identifiers(sql: str) -> List[Tuple[str, str]]:\n",
        "    s = re.sub(r\"\\s+\", \" \", (sql or \"\").strip())\n",
        "    alias_map = extract_alias_map(s)\n",
        "    pairs = []\n",
        "    for m in re.finditer(r\"\\b([a-zA-Z_]\\w*)\\.([a-zA-Z_]\\w*)\\b\", s):\n",
        "        alias = m.group(1)\n",
        "        col = m.group(2)\n",
        "        table = alias_map.get(alias)\n",
        "        if table:\n",
        "            pairs.append((table, col))\n",
        "    return pairs\n",
        "\n",
        "\n",
        "def validate_columns(engine, sql: str) -> None:\n",
        "    pairs = _parse_qualified_identifiers(sql)\n",
        "    cache: Dict[str, set] = {}\n",
        "    for table, col in pairs:\n",
        "        if table not in cache:\n",
        "            cache[table] = get_table_columns(engine, table)\n",
        "        if col not in cache[table]:\n",
        "            raise ValueError(f\"Unknown column, {table}.{col}\")\n",
        "\n",
        "\n",
        "def remove_invalid_hospital_filters(engine, sql: str) -> str:\n",
        "    alias_map = extract_alias_map(sql)\n",
        "    bad_aliases = []\n",
        "    for alias, table in alias_map.items():\n",
        "        cols = get_table_columns(engine, table)\n",
        "        if \"hospital_id\" not in cols:\n",
        "            bad_aliases.append(alias)\n",
        "\n",
        "    if not bad_aliases:\n",
        "        return sql\n",
        "\n",
        "    fixed = sql\n",
        "    for a in bad_aliases:\n",
        "        fixed = re.sub(rf\"\\s+AND\\s+{a}\\.hospital_id\\s*=\\s*:hospital_id\\b\", \"\", fixed, flags=re.IGNORECASE)\n",
        "        fixed = re.sub(rf\"\\b{a}\\.hospital_id\\s*=\\s*:hospital_id\\s+AND\\s+\", \"\", fixed, flags=re.IGNORECASE)\n",
        "        fixed = re.sub(rf\"\\b{a}\\.hospital_id\\s*=\\s*:hospital_id\\b\", \"TRUE\", fixed, flags=re.IGNORECASE)\n",
        "\n",
        "    fixed = re.sub(r\"WHERE\\s+TRUE\\s+AND\\s+\", \"WHERE \", fixed, flags=re.IGNORECASE)\n",
        "    fixed = re.sub(r\"WHERE\\s+TRUE\\s*$\", \"\", fixed, flags=re.IGNORECASE)\n",
        "    return fixed\n",
        "\n",
        "\n",
        "def execute_sql(engine, sql: str, hospital_id: int, extra_params: Optional[Dict[str, Any]] = None, max_rows: int = 200) -> List[Dict[str, Any]]:\n",
        "    wrapped = f\"WITH q AS ({sql}) SELECT * FROM q LIMIT :_limit\"\n",
        "    params: Dict[str, Any] = {\"hospital_id\": hospital_id, \"_limit\": max_rows}\n",
        "    if extra_params:\n",
        "        params.update(extra_params)\n",
        "\n",
        "    with engine.connect() as conn:\n",
        "        rows = conn.execute(text(wrapped), params).mappings().all()\n",
        "    return [dict(r) for r in rows]\n",
        "\n",
        "\n",
        "def summarize_rows(rows: List[Dict[str, Any]]) -> str:\n",
        "    if not rows:\n",
        "        return \"No matching records found.\"\n",
        "\n",
        "    cols = list(rows[0].keys())\n",
        "\n",
        "    if len(rows) <= 10:\n",
        "        return \"\\n\".join([\", \".join([f\"{k}: {r.get(k)}\" for k in cols]) for r in rows])\n",
        "\n",
        "    head = f\"Summary, {len(rows)} rows returned. Showing first 10.\"\n",
        "    body = \"\\n\".join([\", \".join([f\"{k}: {r.get(k)}\" for k in cols]) for r in rows[:10]])\n",
        "    return head + \"\\n\" + body\n",
        "\n",
        "\n",
        "def build_gemini_model() -> Any:\n",
        "    api_key = os.getenv(\"GEMINI_API_KEY\", \"\")\n",
        "    if not api_key:\n",
        "        raise ValueError(\"Missing GEMINI_API_KEY env var\")\n",
        "    genai.configure(api_key=api_key)\n",
        "    model_name = os.getenv(\"GEMINI_SQL_MODEL\", \"models/gemini-2.0-flash-lite-001\")\n",
        "    return genai.GenerativeModel(model_name)\n",
        "\n",
        "\n",
        "_INTENTS = [\n",
        "    (\"ADMISSIONS_BY_DEPARTMENT\", [r\"\\badmit\", r\"\\bdepartment\\b\"], [\"admissions\", \"departments\"]),\n",
        "    (\"ADMISSIONS_DAILY_SERIES\", [r\"\\badmit\", r\"last\\s+\\d+\\s+days|last\\s+7\\s+days|daily\"], [\"admissions\"]),\n",
        "    (\"ADMISSIONS_TODAY\", [r\"\\badmit\", r\"\\btoday\\b\"], [\"admissions\"]),\n",
        "    (\"CURRENT_ADMITTED_LIST\", [r\"\\bcurrently\\b|\\bcurrent\\b\", r\"\\badmitted\\b\"], [\"admissions\", \"patients\", \"departments\", \"wards\", \"rooms\", \"beds\"]),\n",
        "    (\"DISCHARGES\", [r\"\\bdischarg\"], [\"discharges\", \"admissions\", \"patients\"]),\n",
        "    (\"CRITICAL_PATIENTS\", [r\"\\bcritical\\b\"], [\"patient_conditions\", \"admissions\", \"patients\", \"departments\"]),\n",
        "    (\"VITALS\", [r\"\\bvitals?\\b\"], [\"patient_vitals\", \"admissions\", \"patients\"]),\n",
        "    (\"BIRTHS\", [r\"\\bbirths?\\b|\\bbabies\\b\"], [\"births\", \"newborns\", \"patients\"]),\n",
        "    (\"ABORTION\", [r\"\\babortion\\b\"], [\"abortion_cases\", \"patients\", \"staff\"]),\n",
        "    (\"STAFF_COUNTS\", [r\"\\bstaff\\b\", r\"\\brole\\b|by\\s+role|distribution\"], [\"staff\", \"staff_roles\", \"staff_department\", \"departments\"]),\n",
        "    (\"SHIFTS\", [r\"\\bshift\\b|\\bavailability\\b|on\\s+shift\"], [\"staff_shifts\", \"staff\", \"staff_roles\", \"departments\"]),\n",
        "    (\"PRESCRIPTIONS\", [r\"\\bprescription|\\bmedication\"], [\"prescriptions\", \"prescription_items\", \"medications\", \"admissions\", \"patients\", \"staff\"]),\n",
        "    (\"NURSE_ASSIGNMENTS\", [r\"\\bnurse\\b\", r\"\\bassign\"], [\"nurse_assignments\", \"admissions\", \"patients\", \"staff\", \"wards\", \"rooms\", \"beds\"]),\n",
        "    (\"BLOOD_BANK\", [r\"\\bblood\\b\"], [\"blood_bank_inventory\", \"blood_groups\", \"donors\", \"donations\", \"patients\"]),\n",
        "    (\"ORGAN_DONATION\", [r\"\\borgan\\b\"], [\"donations\", \"organ_donation_items\", \"organs\", \"donors\"]),\n",
        "    (\"SURGERY\", [r\"\\bsurger\"], [\"surgeries\", \"surgery_team\", \"admissions\", \"patients\", \"staff\"]),\n",
        "]\n",
        "\n",
        "\n",
        "def detect_intent(question: str) -> Tuple[str, List[str]]:\n",
        "    q = (question or \"\").lower()\n",
        "    for name, patterns, must_tables in _INTENTS:\n",
        "        ok = True\n",
        "        for p in patterns:\n",
        "            if not re.search(p, q):\n",
        "                ok = False\n",
        "                break\n",
        "        if ok:\n",
        "            return name, must_tables\n",
        "    return \"GENERAL\", []\n",
        "\n",
        "\n",
        "def _parse_iso_dt(s: str) -> Optional[datetime]:\n",
        "    s2 = (s or \"\").strip()\n",
        "    s2 = s2.replace(\" \", \"T\")\n",
        "    try:\n",
        "        dt = datetime.fromisoformat(s2)\n",
        "        return dt\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "def get_db_now(engine) -> Optional[datetime]:\n",
        "    try:\n",
        "        with engine.connect() as conn:\n",
        "            dt = conn.execute(text(\"SELECT max(admitted_ts) FROM admissions\")).scalar()\n",
        "            return dt\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "def resolve_time_window(question: str, engine) -> Dict[str, Any]:\n",
        "    q = (question or \"\").lower()\n",
        "\n",
        "    db_now = get_db_now(engine)\n",
        "    if db_now is not None:\n",
        "        now = db_now\n",
        "    else:\n",
        "        now = datetime.now(_DHAKA_TZ) if _DHAKA_TZ else datetime.now()\n",
        "\n",
        "    m_iso = re.search(r\"(\\d{4}-\\d{2}-\\d{2}\\s+\\d{2}:\\d{2}:\\d{2}(?:\\+\\d{2}:\\d{2})?)\", question or \"\")\n",
        "    if m_iso:\n",
        "        dt = _parse_iso_dt(m_iso.group(1))\n",
        "        if dt:\n",
        "            return {\"at_ts\": dt}\n",
        "\n",
        "    if re.search(r\"\\btoday\\b\", q):\n",
        "        start = now.replace(hour=0, minute=0, second=0, microsecond=0)\n",
        "        end = start + timedelta(days=1)\n",
        "        return {\"start_ts\": start, \"end_ts\": end}\n",
        "\n",
        "    if re.search(r\"\\bthis\\s+month\\b\", q):\n",
        "        start = now.replace(day=1, hour=0, minute=0, second=0, microsecond=0)\n",
        "        if start.month == 12:\n",
        "            end = start.replace(year=start.year + 1, month=1)\n",
        "        else:\n",
        "            end = start.replace(month=start.month + 1)\n",
        "        return {\"start_ts\": start, \"end_ts\": end}\n",
        "\n",
        "    m = re.search(r\"last\\s+(\\d+)\\s+days\", q)\n",
        "    if m:\n",
        "        n = int(m.group(1))\n",
        "        end = now\n",
        "        start = now - timedelta(days=n)\n",
        "        return {\"start_ts\": start, \"end_ts\": end}\n",
        "\n",
        "    if re.search(r\"last\\s+7\\s+days\", q):\n",
        "        end = now\n",
        "        start = now - timedelta(days=7)\n",
        "        return {\"start_ts\": start, \"end_ts\": end}\n",
        "\n",
        "    if re.search(r\"last\\s+30\\s+days\", q):\n",
        "        end = now\n",
        "        start = now - timedelta(days=30)\n",
        "        return {\"start_ts\": start, \"end_ts\": end}\n",
        "\n",
        "    return {}\n",
        "\n",
        "\n",
        "def generate_sql(model, question: str, schema_hits: List[Tuple[str, str]], intent: str, time_info: Dict[str, Any], error_hint: str = \"\") -> str:\n",
        "    schema_context = build_schema_context(schema_hits)\n",
        "\n",
        "    time_bucket = \"time\" if time_info else \"notime\"\n",
        "    cache_key = f\"{intent}:{hash(schema_context)}:{time_bucket}\"\n",
        "\n",
        "    if cache_key in SQL_CACHE and not error_hint:\n",
        "        return SQL_CACHE[cache_key]\n",
        "\n",
        "    time_rules = []\n",
        "    if \"start_ts\" in time_info and \"end_ts\" in time_info:\n",
        "        time_rules.append(\"If the question is time based, use :start_ts and :end_ts for filtering on the correct timestamp column.\")\n",
        "    if \"at_ts\" in time_info:\n",
        "        time_rules.append(\"If the question asks availability at a time, use :at_ts and compare it between shift start and end timestamps.\")\n",
        "    time_rule_text = \"\\n\".join(time_rules) if time_rules else \"If the question is time based, you may use NOW() but prefer provided parameters when available.\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You generate PostgreSQL SQL to be executed with SQLAlchemy.\n",
        "\n",
        "Rules\n",
        "1 Output exactly one SQL query\n",
        "2 Only SELECT or WITH is allowed\n",
        "3 Use only tables and columns present in the schema context\n",
        "4 Apply hospital filter only on tables that actually contain a hospital_id column\n",
        "5 Add LIMIT 200 for list queries\n",
        "6 Do not output semicolons or code fences\n",
        "7 Use :hospital_id placeholder, not a literal number\n",
        "8 {time_rule_text}\n",
        "9 Prefer join paths shown in common_joins if present\n",
        "10 When querying patient_conditions for current status, select the latest record per admission using max(updated_ts)\n",
        "\n",
        "Intent\n",
        "{intent}\n",
        "\n",
        "Schema context\n",
        "{schema_context}\n",
        "\n",
        "Question\n",
        "{question}\n",
        "\n",
        "{error_hint}\n",
        "\n",
        "Output only SQL\n",
        "\"\"\".strip()\n",
        "\n",
        "    sql = sanitize_sql(safe_generate(model, prompt))\n",
        "    if not error_hint:\n",
        "        SQL_CACHE[cache_key] = sql\n",
        "    return sql\n",
        "\n",
        "\n",
        "_ENGINE = None\n",
        "_SCHEMA_COL = None\n",
        "_EMBEDDER = None\n",
        "_GEMINI = None\n",
        "\n",
        "COLLECTION_NAME = os.getenv(\"MILVUS_COLLECTION\", \"db_schema_docs\")\n",
        "EMBED_DIM = int(os.getenv(\"EMBED_DIM\", \"768\"))\n",
        "\n",
        "\n",
        "def init_once(index: bool = False) -> None:\n",
        "    global _ENGINE, _SCHEMA_COL, _EMBEDDER, _GEMINI\n",
        "\n",
        "    if _ENGINE is None:\n",
        "        _ENGINE = build_engine()\n",
        "    if _SCHEMA_COL is None:\n",
        "        _SCHEMA_COL = build_milvus_collection(COLLECTION_NAME, EMBED_DIM)\n",
        "    if _EMBEDDER is None:\n",
        "        _EMBEDDER = build_embedder()\n",
        "    if _GEMINI is None:\n",
        "        _GEMINI = build_gemini_model()\n",
        "\n",
        "    if index:\n",
        "        index_schema(_ENGINE, _SCHEMA_COL, _EMBEDDER, schema_version=os.getenv(\"SCHEMA_VERSION\", \"v1\"))\n",
        "\n",
        "\n",
        "def chat(question: str, hospital_id: int) -> str:\n",
        "    init_once(index=False)\n",
        "\n",
        "    intent, must_tables = detect_intent(question)\n",
        "    time_info = resolve_time_window(question, _ENGINE)\n",
        "\n",
        "    schema_hits = retrieve_schema(_SCHEMA_COL, _EMBEDDER, question, top_k=12)\n",
        "    if must_tables:\n",
        "        schema_hits = _ensure_tables(_SCHEMA_COL, _EMBEDDER, schema_hits, must_tables)\n",
        "\n",
        "    allowed_tables = [t for t, _ in schema_hits]\n",
        "\n",
        "    extra_params: Dict[str, Any] = {}\n",
        "    if \"start_ts\" in time_info and \"end_ts\" in time_info:\n",
        "        extra_params[\"start_ts\"] = time_info[\"start_ts\"]\n",
        "        extra_params[\"end_ts\"] = time_info[\"end_ts\"]\n",
        "    if \"at_ts\" in time_info:\n",
        "        extra_params[\"at_ts\"] = time_info[\"at_ts\"]\n",
        "\n",
        "    global _LAST_LLM_CALL_TS\n",
        "    now_ts = time.time()\n",
        "    gap = now_ts - _LAST_LLM_CALL_TS\n",
        "    if gap < 1.2:\n",
        "        time.sleep(1.2 - gap)\n",
        "    _LAST_LLM_CALL_TS = time.time()\n",
        "\n",
        "    sql = generate_sql(_GEMINI, question, schema_hits, intent=intent, time_info=time_info)\n",
        "    sql = remove_invalid_hospital_filters(_ENGINE, sql)\n",
        "\n",
        "    try:\n",
        "        validate_sql(sql, allowed_tables=allowed_tables)\n",
        "        validate_columns(_ENGINE, sql)\n",
        "    except Exception as e:\n",
        "        hint = f\"Previous SQL failed validation because: {str(e)}. Regenerate SQL using only valid identifiers from schema context.\"\n",
        "        sql = generate_sql(_GEMINI, question, schema_hits, intent=intent, time_info=time_info, error_hint=hint)\n",
        "        sql = remove_invalid_hospital_filters(_ENGINE, sql)\n",
        "        validate_sql(sql, allowed_tables=allowed_tables)\n",
        "        validate_columns(_ENGINE, sql)\n",
        "\n",
        "    rows = execute_sql(_ENGINE, sql, hospital_id=hospital_id, extra_params=extra_params, max_rows=200)\n",
        "    return summarize_rows(rows)"
      ]
    }
  ]
}